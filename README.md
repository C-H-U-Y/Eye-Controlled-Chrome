# Eye-Controlled-Chrome
Selenium powered web instance that interprets brain waves from eye movements read by Backyard Brains 'brainbox', classifies the eye movements as left, right or blink and then passes those instructions as commands to chrome via selenium.

## Outcome
Developed as a social tool to connect those with restricted lower body and limb movement to social netwroking, this human-machine interface achieved 100% accuracy in the final product demonstration. Successfuly logging in to facebook, navigating to a groupchat and sending an emoji, all commanded by eye movements. The engineering of this system overcame many challenges such as:
- The need for text controls and navigation controls overcome by introducing dynamic mode shifting
- Linking selenium up with live input signals overcome by utilising concurrent processes and asynchronous programming
- Mapping eye movements to text was overcome by implement a practical morse code solution

Ultimately the product achieved full marks for it's robust, impressive solution to a real world problem.

## Workflow Diagram
![image](https://user-images.githubusercontent.com/117706300/202077363-f0ef746c-4591-43af-8721-25dcba930f62.png)

## User Manual
To see the full capability of the project and instructions on how to use it yourself, checkout the user manual:

[User Manual](https://github.com/C-H-U-Y/Eye-Controlled-Chrome/blob/main/User%20Manual.pdf)

## Functional Demo
https://user-images.githubusercontent.com/117706300/202076059-142387fc-67f4-4fb8-b6e8-dd1ba76f4d7a.mp4

